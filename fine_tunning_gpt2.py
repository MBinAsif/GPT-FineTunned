# -*- coding: utf-8 -*-
"""Fine-Tunning-Gpt2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pYZYaT02EuT_P2TDzek4zPSgADSAfFgw
"""

import torch
from datasets import load_dataset
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import os
# Load the dataset
ds = load_dataset("argilla/llama-2-banking-fine-tune")

text_column_name = 'request'

# Tokenization and preprocessing
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Adding a padding token

max_length = 512  # Set your desired maximum sequence length

# Filter out long sequences and tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples[text_column_name], padding="max_length", truncation=True, max_length=max_length)

tokenized_ds = ds["train"].map(tokenize_function, batched=True)

# Create a smaller subset of the tokenized dataset
small_tokenized_ds = tokenized_ds.select(range(100))  # Selecting a smaller subset

# Prepare model
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Check and adjust the model's vocabulary size to match the tokenizer
if len(tokenizer) != model.config.vocab_size:
    model.resize_token_embeddings(len(tokenizer))

# Training settings
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)

# Training loop on the smaller dataset
for epoch in range(1):  # Adjust the number of epochs
    model.train()
    total_loss = 0
    for batch in small_tokenized_ds:
        input_ids = torch.tensor(batch["input_ids"]).to(device)
        attention_mask = torch.tensor(batch["attention_mask"]).to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)
        loss = outputs.loss

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    average_loss = total_loss / len(small_tokenized_ds)
    print(f"Epoch {epoch + 1}, Average Loss: {average_loss}")

from google.colab import drive
import os

# Mount Google Drive
drive.mount('/content/drive')

# Directory path in Google Drive where you want to save the model
save_directory = '/content/drive/My Drive/MyModels'

# Create the directory if it doesn't exist
if not os.path.exists(save_directory):
    os.makedirs(save_directory)

# Assuming 'model' is your fine-tuned GPT-2 model
model = GPT2LMHeadModel.from_pretrained("gpt2")  # Load your fine-tuned model here

# Save the model to the specified directory
model.save_pretrained(save_directory)

# Optionally, save the tokenizer too
tokenizer.save_pretrained(save_directory)

from transformers import GPT2Tokenizer, GPT2LMHeadModel

# Load the fine-tuned model and tokenizer
model_directory = '/content/drive/My Drive/MyModels'  # Update with your saved model directory
tokenizer = GPT2Tokenizer.from_pretrained(model_directory)
model = GPT2LMHeadModel.from_pretrained(model_directory)

# Set the device (CPU or GPU) for inference
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

import torch

# Example text generation
prompt_text = "The banking service was"
input_ids = tokenizer.encode(prompt_text, return_tensors='pt').to(device)

# Generate text based on the prompt
output = model.generate(input_ids, max_length=100, num_return_sequences=1, temperature=0.7)

# Decode and print the generated text
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
print("Generated Text:")
print(generated_text)